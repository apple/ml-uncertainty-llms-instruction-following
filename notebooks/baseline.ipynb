{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results of baseline uncertainty estimation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# For licensing see accompanying LICENSE file.\n",
    "# Copyright (C) 2025 Apple Inc. All Rights Reserved.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def readjsonl(datapath):\n",
    "    res = []\n",
    "    with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            res.append(json.loads(line))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_auroc(_response_df, uncertainty_type='perplexity', task_type='Controlled_Easy'):\n",
    "    dic = {}\n",
    "    if task_type == 'Controlled_Easy':\n",
    "        label_list = ['correct', 'incorrect']\n",
    "    elif task_type == 'Controlled_Hard':\n",
    "        label_list = ['correct', 'subtle_off']\n",
    "    elif task_type == 'Realistic':\n",
    "        label_list = ['correct', 'incorrect']\n",
    "    else:\n",
    "        raise ValueError('task_type should be one of [Controlled-Easy, Controlled-Hard, Realistic]')\n",
    "\n",
    "    for following_label in label_list:\n",
    "        dic[following_label] = _response_df[_response_df['following_label']==following_label][uncertainty_type].apply(lambda x: float(x))\n",
    "\n",
    "    # // Compute AUROC\n",
    "    gt_labels_list = []\n",
    "    binary_uncertainty_list = []\n",
    "    for label in label_list:\n",
    "        if uncertainty_type in ['verbalized_confidence', 'normalized_p_true', 'p_true']: # // <-- higher is better\n",
    "            gt_labels_list.append([int(label=='correct')]*len(dic[label])) # // <-- 0: incorrect 1: correct\n",
    "        elif uncertainty_type in ['perplexity', 'entropy','maximum_seq_prob']:  # // <-- lower is better\n",
    "            gt_labels_list.append([int(label!='correct')]*len(dic[label])) # // <-- 0: correct 1: incorrect\n",
    "        binary_uncertainty_list.append(dic[label])\n",
    "    gt_labels_list = np.concatenate(gt_labels_list)\n",
    "    binary_uncertainty_list = np.concatenate(binary_uncertainty_list)\n",
    "\n",
    "    # // Replace nan\n",
    "    binary_uncertainty_list = np.array(binary_uncertainty_list, dtype=float)\n",
    "    nan_mask = ~np.isnan(binary_uncertainty_list)\n",
    "    gt_labels_list = gt_labels_list[nan_mask]\n",
    "    binary_uncertainty_list = binary_uncertainty_list[nan_mask]\n",
    "\n",
    "    print(dic, gt_labels_list, binary_uncertainty_list)\n",
    "\n",
    "    return gt_labels_list, binary_uncertainty_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check all uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='Llama-2-7b-chat-hf' # Llama-2-7b-chat-hf Phi-3-mini-128k-instruct Mistral-7B-Instruct-v0.3\n",
    "task_type='Controlled_Easy'\n",
    "data_path = ''\n",
    "\n",
    "data_type = 'controlled_ver' if 'Cont' in task_type else 'reality_ver'\n",
    "task_path = f\"{data_path}/{MODEL}/{data_type}/\"\n",
    "response_path = os.path.join(task_path, \"all_eval_response_and_baseline.jsonl\")\n",
    "_response_df = pd.DataFrame(readjsonl(response_path))\n",
    "\n",
    "for uncertainty_type in ['perplexity', 'entropy', 'normalized_p_true', 'maximum_seq_prob', 'verbalized_confidence', 'p_true']:\n",
    "    print()\n",
    "    print('------', uncertainty_type, '------')\n",
    "    # // Select insturctions\n",
    "    inst_dic = {}\n",
    "    all_inst = ['startend', 'detectable_content', 'detectable_format', 'language', 'change_case', 'keywords', 'length_constraints',  'punctuation']\n",
    "    ind_list = []\n",
    "    for inst in all_inst:\n",
    "        for i in range(len(_response_df)):\n",
    "            category = _response_df['instruction_id_list'][i][0].split(':')[0]\n",
    "            if inst == category:\n",
    "                ind_list.append(i)\n",
    "        _response_df_inst = _response_df.iloc[ind_list]\n",
    "\n",
    "        gt_labels_list, binary_uncertainty_list = compute_perplexity_auroc( _response_df_inst, uncertainty_type, task_type)\n",
    "\n",
    "        # // Stat of test\n",
    "        succ = (gt_labels_list==0).sum()\n",
    "        fail = (gt_labels_list==1).sum()\n",
    "\n",
    "        # // Mask inf\n",
    "        inf_mask = ~np.isinf(binary_uncertainty_list)\n",
    "        gt_labels_list = gt_labels_list[inf_mask]\n",
    "        binary_uncertainty_list = binary_uncertainty_list[inf_mask]\n",
    "\n",
    "\n",
    "        roc = roc_auc_score(gt_labels_list, binary_uncertainty_list)\n",
    "        print(inst)\n",
    "        print(roc)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
